# 基准测试脚本文档

## 概述

本基准测试脚本用于测量 Transformer 模型的前向传播（forward pass）和后向传播（backward pass）的执行时间。脚本支持通过命令行参数灵活配置模型超参数和测试参数，便于进行不同配置下的性能对比。

## 测试内容

脚本主要测量以下内容：

1. **前向传播时间**：模型处理输入数据并生成输出的时间
2. **后向传播时间**：计算梯度的时间（可选）
3. **统计信息**：包括平均值、标准差、最小值和最大值

## 需要测试的模型维度

根据作业要求，本基准测试需要覆盖以下5种模型大小配置，以了解不同规模下的性能表现：

| 模型大小 | d_model | d_ff | num_layers | num_heads |
|---------|---------|------|------------|-----------|
| small   | 768     | 3072 | 12         | 12        |
| medium  | 1024    | 4096 | 24         | 16        |
| large   | 1280    | 5120 | 36         | 20        |
| xl      | 1600    | 6400 | 48         | 25        |
| 2.7B    | 2560    | 10240| 32         | 32        |

**说明**：
- 所有模型配置使用统一的词汇表大小（vocab_size = 10000）和批次大小（batch_size = 4）
- 上下文长度（context_length）可以根据需要变化，用于测试不同序列长度对性能的影响
- 建议对每种模型大小分别测试前向传播和后向传播的性能，并记录平均值和标准差
- 通过对比不同模型大小的性能数据，可以分析模型规模与计算时间的关系，为实际训练选择合适的模型配置提供参考

## 使用方法

### 基本用法

```bash
uv run python cs336_systems/benchmark/benchmark.py \
    --context_length 512 \
    --d_model 768 \
    --num_layers 12 \
    --num_heads 12 \
    --d_ff 3072
```

### 主要参数

- **模型超参数**（必需）：
  - `--context_length`: 上下文长度（序列长度）
  - `--d_model`: 模型维度
  - `--num_layers`: Transformer 层数
  - `--num_heads`: 注意力头数
  - `--d_ff`: 前馈网络维度

- **测试参数**（可选）：
  - `--vocab_size`: 词汇表大小（默认：10000）
  - `--batch_size`: 批次大小（默认：4）
  - `--warmup_steps`: 预热步数（默认：5）
  - `--num_steps`: 测量步数（默认：10）
  - `--forward_only`: 仅运行前向传播
  - `--device`: 运行设备（默认：cuda）
  - `--rope_theta`: RoPE 位置编码的 theta 参数（默认：10000.0）

### 示例：测试不同模型大小

```bash
# Small 模型
uv run python cs336_systems/benchmark/benchmark.py \
    --context_length 512 --d_model 768 --num_layers 12 --num_heads 12 --d_ff 3072

# Medium 模型
uv run python cs336_systems/benchmark/benchmark.py \
    --context_length 512 --d_model 1024 --num_layers 24 --num_heads 16 --d_ff 4096

# Large 模型
uv run python cs336_systems/benchmark/benchmark.py \
    --context_length 512 --d_model 1280 --num_layers 36 --num_heads 20 --d_ff 5120

# XL 模型
uv run python cs336_systems/benchmark/benchmark.py \
    --context_length 512 --d_model 1600 --num_layers 48 --num_heads 25 --d_ff 6400

# 2.7B 模型
uv run python cs336_systems/benchmark/benchmark.py \
    --context_length 512 --d_model 2560 --num_layers 32 --num_heads 32 --d_ff 10240
```

## 核心方法说明

### 1. Warm-up Steps（预热步骤）

**为什么需要 warm-up steps？**

1. **GPU 初始化开销**：首次执行 CUDA 操作时，GPU 需要初始化驱动、加载内核、分配内存等，这些一次性开销会显著增加首次运行时间。

2. **运行时优化**：PyTorch 和 CUDA 会在运行时进行各种优化，包括：
   - 内存分配策略的建立
   - 内核自动调优和选择
   - 缓存机制的建立
   
   这些优化需要几次运行才能稳定下来。

3. **消除冷启动效应**：确保测量的时间反映的是实际计算时间，而不是初始化开销。没有 warm-up 时，第一次测量的时间会包含这些一次性开销，导致结果不准确且不稳定。

**影响**：经过几次 warm-up 后，GPU 状态稳定，内存分配模式建立，后续的运行时间会更一致和准确。通常建议使用 5-10 个 warm-up 步骤。

### 2. torch.cuda.synchronize()

**为什么需要使用 torch.cuda.synchronize()？**

1. **CUDA 异步执行**：CUDA 操作（如 `torch.matmul`）是异步的。当你调用一个 CUDA 内核时，函数会立即返回控制权给 CPU，而实际的矩阵乘法在 GPU 上继续执行。这意味着 CPU 可以在 GPU 计算的同时继续执行其他代码。

2. **计时准确性问题**：如果不使用 `synchronize()`，直接测量函数调用返回的时间，实际上测量的是 CPU 发出指令的时间，而不是 GPU 真正完成计算的时间。这会导致测量结果严重不准确。

3. **确保测量准确性**：`torch.cuda.synchronize()` 会阻塞 CPU，直到所有 GPU 操作完成。这样我们才能准确测量 GPU 内核的实际运行时间。

**使用位置**：
- 在每次前向/后向传播之前：确保之前的操作已完成
- 在每次前向/后向传播之后：等待当前操作完成后再计时

### 3. timeit.default_timer()

脚本使用 `timeit.default_timer()` 而不是 `time.time()`，因为：
- `default_timer()` 提供系统最高分辨率的时钟
- 更适合进行精确的性能基准测试
- 在不同平台上自动选择最合适的计时器

### 4. 随机数据生成

脚本使用随机生成的 token IDs 作为输入数据，因为：
- 基准测试关注的是速度而非准确性
- 随机数据可以避免数据加载的开销
- 确保每次测试的数据分布一致

## 内存占用计算

### calculate_memory.py 脚本

`calculate_memory.py` 脚本用于计算 Transformer 模型在训练时的内存占用，支持 float32 精度下的详细内存分析。

#### 使用方法

直接运行脚本即可计算 2.7B 模型的内存占用：

```bash
uv run python cs336_systems/benchmark/calculate_memory.py
```

#### 功能说明

脚本会计算以下内存占用：

1. **模型参数内存**：
   - Embedding 层参数
   - 所有 Transformer Block 的参数（Attention、FFN、LayerNorm）
   - 最终 LayerNorm 和 LM Head 参数

2. **梯度内存**：
   - 与模型参数相同大小的梯度存储

3. **优化器状态内存**（AdamW）：
   - Momentum：参数大小
   - Variance：参数大小
   - 总计：2 × 参数大小

4. **激活值内存**：
   - 输入 embedding 激活值
   - 每层的激活值（层输入、注意力矩阵、FFN 中间激活值等）
   - 输出 logits

#### 输出示例

脚本会输出详细的内存占用分解：

```
================================================================================
2.7B模型内存占用计算 (float32)
================================================================================

内存占用明细
================================================================================

1. 模型参数:
   总参数数: 3,406,809,600
   内存占用: 12.69 GB

2. 梯度内存:
   内存占用: 12.69 GB

3. 优化器状态 (AdamW):
   内存占用: 25.38 GB
   (momentum + variance = 2 × 参数大小)

4. 激活值内存 (峰值):
   峰值激活值: 39,600,128 元素
   内存占用: 0.15 GB

5. 总内存占用 (峰值):
   54.00 GB
```

#### 计算说明

- **模型参数计算**：精确计算所有层的参数数量，包括 Attention 的 QKV 投影和输出投影、FFN 的 SwiGLU 层（w1、w2、w3）、以及所有 LayerNorm 参数
- **激活值计算**：考虑不使用梯度检查点的情况，需要保存所有层的激活值用于反向传播
- **优化器状态**：基于 AdamW 优化器，需要存储 momentum 和 variance 两个状态

#### 注意事项

1. **激活值内存**：计算的是不使用梯度检查点的情况。如果使用梯度检查点，激活值内存会显著减少（只需要保存当前层的激活值）
2. **精度**：当前脚本计算的是 float32 精度（4 bytes/param）。如果使用混合精度训练（如 bfloat16），内存占用会显著减少
3. **临时缓冲区**：实际训练中可能还需要额外的临时缓冲区内存，这里未包含在内
4. **单 GPU 训练**：计算的是单 GPU 训练的内存占用。如果使用分布式训练，需要考虑通信开销

#### 2.7B 模型内存占用总结

根据脚本计算，2.7B 模型在 context_length=256、batch_size=4、float32 精度下的内存占用：

| 组件 | 内存占用 (GB) |
|------|--------------|
| 模型权重 | 12.69 |
| 梯度 | 12.69 |
| 优化器状态 | 25.38 |
| 激活值（峰值） | 0.15 |
| **总计** | **54.00** |

**结论**：2.7B 模型在 float32 精度下进行训练，至少需要约 **54 GB** 的 GPU 内存。如果使用梯度检查点和混合精度训练，内存占用可以显著降低。

## 基准测试结果

### 测试配置

根据作业要求，对所有5种模型大小进行了基准测试，测试配置如下：
- **Warm-up 步骤**：5 步
- **测量步骤**：10 步
- **批次大小**：4
- **词汇表大小**：10000
- **上下文长度**：512（2.7B 模型由于内存限制使用 256）

### 性能测试结果

| 模型大小 | 上下文长度 | 前向传播时间 (ms) | 后向传播时间 (ms) | 总时间 (ms) | 前向传播标准差 (ms) | 后向传播标准差 (ms) |
|---------|-----------|-----------------|-----------------|------------|-------------------|-------------------|
| small   | 512       | 45.527 ± 0.019  | 93.248 ± 0.067  | 138.776    | 0.019             | 0.067             |
| medium  | 512       | 137.308 ± 1.800 | 283.582 ± 0.584 | 420.890    | 1.800             | 0.584             |
| large   | 512       | 284.192 ± 0.082 | 608.084 ± 0.699 | 892.276    | 0.082             | 0.699             |
| xl      | 512       | 572.918 ± 0.243 | 1191.383 ± 0.380| 1764.300   | 0.243             | 0.380             |
| 2.7B    | 256       | 417.464 ± 2.251 | 904.665 ± 0.334 | 1322.129   | 2.251             | 0.334             |

### 结果分析

1. **前向传播时间**：
   - 随着模型规模增大，前向传播时间显著增加
   - Small 模型：45.5 ms
   - Medium 模型：137.3 ms（约 3 倍）
   - Large 模型：284.2 ms（约 6.2 倍）
   - XL 模型：572.9 ms（约 12.6 倍）
   - 2.7B 模型（context_length=256）：417.5 ms

2. **后向传播时间**：
   - 后向传播时间约为前向传播时间的 2 倍左右
   - 这是因为后向传播需要计算和存储梯度，计算量更大

3. **时间变异性**：
   - 所有模型的标准差都非常小（< 2.3 ms），说明测量结果非常稳定
   - Small、Large 和 XL 模型的标准差尤其小（< 0.7 ms），表明这些模型的性能非常一致
   - Medium 模型的前向传播标准差稍大（1.8 ms），可能是由于模型规模导致的某些优化不稳定
   - 2.7B 模型的前向传播标准差为 2.251 ms，相对较大，可能与内存管理有关

4. **总体观察**：
   - 使用 5 个 warm-up 步骤后，测量结果非常稳定，标准差很小
   - 这表明 warm-up 步骤有效地消除了冷启动效应，获得了准确的性能测量
   - 后向传播的标准差普遍小于前向传播，这可能是因为后向传播的计算模式更加规律

### Warm-up 步骤的影响分析

为了验证 warm-up 步骤的重要性，我们对 small 模型进行了不同 warm-up 步骤数的测试：

| Warm-up 步骤数 | 前向传播时间 (ms) | 前向传播标准差 (ms) | 后向传播时间 (ms) | 后向传播标准差 (ms) |
|--------------|-----------------|-------------------|-----------------|-------------------|
| 0            | 96.223          | **160.046**       | 139.733         | **146.668**       |
| 1            | 45.888          | 0.549             | 93.455          | 0.767             |
| 2            | 45.779          | 0.367             | 93.628          | 0.457             |
| 5            | 45.623          | **0.051**         | 93.386          | **0.046**         |

#### 结果分析

1. **没有 warm-up 步骤（0 步）**：
   - 标准差非常大（前向：160 ms，后向：147 ms），说明测量结果极不稳定
   - 平均时间明显偏大（前向：96 ms vs 正常 45 ms），因为第一次运行包含了初始化开销
   - 最小值和最大值差异巨大（前向：45.6 ms 到 551.7 ms），表明第一次运行非常慢

2. **1-2 个 warm-up 步骤**：
   - 标准差显著减小（从 160 ms 降到 0.5-0.8 ms），但仍比 5 个 warm-up 步骤大
   - 平均时间接近正常值，但仍有轻微波动
   - 说明 1-2 个 warm-up 步骤不足以完全消除冷启动效应

3. **5 个 warm-up 步骤**：
   - 标准差最小（前向：0.051 ms，后向：0.046 ms），测量结果非常稳定
   - 平均时间最准确，反映了模型的真实性能

#### 为什么会出现这种情况？

1. **GPU 初始化开销**：
   - 第一次 CUDA 操作需要初始化驱动、加载内核、分配内存
   - 这些一次性开销会显著增加首次运行时间（可能增加 10-20 倍）

2. **运行时优化需要多次迭代**：
   - PyTorch 和 CUDA 的自动调优需要几次运行才能稳定
   - 内存分配策略、内核选择、缓存机制都需要时间建立
   - 1-2 次 warm-up 可能不足以完成所有优化

3. **为什么 1-2 个 warm-up 步骤仍然不够**：
   - 某些优化（如 CUDA 内核自动调优）可能需要更多次运行
   - 内存分配模式可能需要多次迭代才能稳定
   - GPU 的某些缓存机制可能需要多次访问才能生效

**结论**：使用 5 个 warm-up 步骤是必要的，可以确保测量结果准确且稳定。没有 warm-up 步骤会导致测量结果严重失真，而 1-2 个 warm-up 步骤虽然比没有好，但仍不足以完全消除冷启动效应。



